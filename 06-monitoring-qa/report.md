# Отчёт по проекту: RAG-ассистент Сбербанка

## Название проекта и краткое описание

**Название:** RAG-ассистент Сбербанка

**Описание:** Telegram-бот с RAG (Retrieval-Augmented Generation) для ответов на вопросы по документам Сбербанка о кредитах и вкладах. Бот использует LangChain для индексации PDF-документов и JSON-файлов с готовыми Q&A парами, реализует контекстный диалог с query transformation, отображает источники ответов и поддерживает оценку качества через RAGAS с интеграцией LangSmith.

## Вариант задания

**Расширенный вариант** – реализованы все итерации спринтов 1–3, включая:
- Базовый LLM-бот с историей диалогов
- Полноценный RAG с индексацией PDF и JSON
- Query transformation для уточняющих вопросов
- Отображение источников (опционально)
- LangSmith трейсинг
- Автоматический синтез датасетов
- Оценка качества через RAGAS (6 метрик)
- Загрузка датасетов и feedback в LangSmith

## Используемые модели и провайдеры

### Для RAG (основной пайплайн)
- **Провайдер:** ProxyAPI (прокси к OpenAI-совместимому API)
- **LLM для генерации ответов:** `gpt-4.1`
- **LLM для трансформации запросов:** `gpt-4.1`
- **Модель эмбеддингов:** `text-embedding-3-large`
- **Базовый URL:** `https://api.proxyapi.ru/openai/v1`

### Для RAGAS evaluation
- **LLM для оценки:** `gpt-4.1` (через `RAGAS_LLM_MODEL`)
- **Модель эмбеддингов для оценки:** `text-embedding-3-large` (через `RAGAS_EMBEDDING_MODEL`)

### Другие настройки
- **Размер чанков:** 500 символов
- **Количество извлекаемых чанков (k):** 10 (`RETRIEVER_K=10`)
- **Отображение источников:** включено (`SHOW_SOURCES=true`)

## Создание и загрузка датасета

### Как создавали датасет
Датасет создан автоматически с помощью модуля `dataset_synthesizer.py`:
1. **Синтез Q&A пар из PDF** – для каждого PDF-файла выбирается 2 случайных чанка, LLM генерирует вопрос и ответ на основе содержимого чанка.
2. **Загрузка готовых Q&A пар из JSON** – из файла `sberbank_help_documents.json` берутся 2 случайные пары вопрос‑ответ.
3. **Объединение** – все пары сохраняются в единый JSON-файл.

### Размер датасета
- **Всего примеров:** 6
- **Синтезировано из PDF:** 4 (по 2 из каждого PDF)
- **Загружено из JSON:** 2

### Скриншот страницы датасета в LangSmith
![Датасет в LangSmith](screenshots/dataset_langsmith.png)  
*(Примечание: скриншот должен быть размещён в папке `screenshots/`)*

### Примеры Q&A пар из датасета

**Пример 1 (синтезирован из PDF):**
- **Вопрос:** Кто является сторонами по Договору в условиях потребительского кредита?
- **Ответ (ground_truth):** Сторонами по Договору являются Кредитор и Заемщик или Созаемщики.
- **Контекст:** фрагмент из документа `ouk_potrebitelskiy_kredit_lph.pdf`.

**Пример 2 (из JSON):**
- **Вопрос:** Сколько стоит перевыпуск карты?
- **Ответ:** Комиссия за перевыпуск большинства карт составляет 150 ₽, перевыпуск социальной карты стоит 30 или 60 ₽, а карты с индивидуальным дизайном — 500 ₽. В некоторых случаях карта может быть перевыпущена бесплатно...

## Оценка качества через RAGAS

### Какие метрики используются для оценки качества
В проекте реализованы 6 метрик RAGAS:
1. **Faithfulness (Обоснованность)** – ответ не содержит галлюцинаций и основан только на retrieved документах.
2. **Answer Relevancy (Релевантность ответа)** – ответ релевантен заданному вопросу.
3. **Answer Correctness (Правильность)** – ответ соответствует ground truth эталону.
4. **Answer Similarity (Похожесть)** – семантическая похожесть ответа на эталон.
5. **Context Recall (Полнота контекста)** – retrieved документы содержат информацию для правильного ответа.
6. **Context Precision (Точность поиска)** – retrieved документы релевантны вопросу.

### Результаты последнего evaluation (из логов)
В логах от 2025‑12‑06 18:25:22 видно следующие средние значения метрик на датасете из 6 примеров:
- **faithfulness:** 0.933
- **answer_relevancy:** 0.005
- **answer_correctness:** 0.713
- **answer_similarity:** 0.846
- **context_recall:** 1.000
- **context_precision:** 0.886

**Интерпретация:**
- **Faithfulness** высокий (0.933) – ответы в основном основаны на документах, галлюцинаций мало.
- **Answer Relevancy** очень низкий (0.005) – требует дополнительного анализа; возможно, ответы недостаточно точно соответствуют вопросам.
- **Answer Correctness** средний (0.713) – ответы частично соответствуют эталону.
- **Answer Similarity** хороший (0.846) – семантически ответы близки к ground truth.
- **Context Recall** идеальный (1.000) – retrieved документы всегда содержат нужную информацию.
- **Context Precision** высокий (0.886) – поиск возвращает релевантные документы.

## Выводы

### Главные инсайты о качестве RAG системы
1. **Сильные стороны:**
   - Система успешно извлекает релевантные документы (Context Recall = 1.000, Context Precision = 0.886).
   - Ответы почти не содержат галлюцинаций (Faithfulness = 0.933).
   - Семантическая близость ответов к эталону высокая (Answer Similarity = 0.846).

2. **Области для улучшения:**
   - **Крайне низкая Answer Relevancy (0.005)** указывает на то, что ответы часто не отвечают прямо на поставленный вопрос. Возможно, требуется настройка промптов или улучшение query transformation.
   - **Answer Correctness (0.713)** можно повысить, уточняя ground truth или улучшая генерацию ответов.

3. **Техническая стабильность:**
   - Индексация работает стабильно (589 чанков из 2 PDF и 1 JSON).
   - Бот корректно обрабатывает команды и вопросы в реальном времени.
   - LangSmith трейсинг и загрузка feedback работают без ошибок.

4. **Рекомендации:**
   - Провести анализ низкой Answer Relevancy: возможно, нужно увеличить количество retrieved чанков (k) или доработать промпт.
   - Расширить датасет для более репрезентативной оценки.
   - Экспериментировать с разными моделями эмбеддингов для улучшения поиска.

## Заключение
Проект успешно реализует расширенный вариант RAG-ассистента с полным циклом оценки качества. Система готова к использованию, но требует дальнейшей тонкой настройки метрик релевантности и правильности ответов.